<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
	<meta name="keywords" content="contents" />
	<meta name="description" content="contents" />
	<!-- 网页标签标题 -->
	<title>contents</title>
	<link rel="shortcut icon" href="/img/aimatrix.ico"/>
	<link rel="stylesheet" href="/build/documentation.css" />
</head>
<body>
	<div id="root"><div class="documentation-page" data-reactroot="" data-reactid="1" data-react-checksum="551392803"><header class="header-container header-container-normal" data-reactid="2"><div class="header-body" data-reactid="3"><a href="/en-us/index.html" data-reactid="4"><img class="logo" src="/img/ai_small_green.png" data-reactid="5"/></a><span class="language-switch language-switch-normal" data-reactid="6">中</span><div class="header-menu" data-reactid="7"><img class="header-menu-toggle" src="/img/system/menu_gray.png" data-reactid="8"/><ul data-reactid="9"><li class="menu-item menu-item-normal" data-reactid="10"><a href="/en-us/index.html" target="_self" data-reactid="11">HOME</a></li><li class="menu-item menu-item-normal menu-item-normal-active" data-reactid="12"><a href="/en-us/docs/goals.html" target="_self" data-reactid="13">DOCS</a></li></ul></div></div></header><div class="bar" data-reactid="14"><div class="bar-body" data-reactid="15"><img src="/img/system/docs.png" class="front-img" data-reactid="16"/><span data-reactid="17">Documentation</span><img src="/img/system/docs.png" class="back-img" data-reactid="18"/></div></div><section class="content-section" data-reactid="19"><div class="sidemenu" data-reactid="20"><div class="sidemenu-toggle" data-reactid="21"><img src="https://img.alicdn.com/tfs/TB1E6apXHGYBuNjy0FoXXciBFXa-200-200.png" data-reactid="22"/></div><ul data-reactid="23"><li class="menu-item menu-item-level-1" data-reactid="24"><span data-reactid="25">Documentation</span><ul data-reactid="26"><li style="height:36px;overflow:hidden;" class="menu-item menu-item-level-2" data-reactid="27"><a href="/en-us/docs/goals.html" target="_self" data-reactid="28">Goals</a></li><li style="height:36px;overflow:hidden;" class="menu-item menu-item-level-2" data-reactid="29"><a href="/en-us/docs/contents.html" target="_self" data-reactid="30">Contents</a></li><li style="height:36px;overflow:hidden;" class="menu-item menu-item-level-2" data-reactid="31"><a href="/en-us/docs/metrics.html" target="_self" data-reactid="32">Metrics</a></li></ul></li></ul></div><div class="doc-content markdown-body" data-reactid="33"><h1>Benchmarks</h1>
<h3>Micro Benchmarks</h3>
<p>The micro benchmark category consists of tests that measure the performance of the basic operations involved in training deep learning neural networks. This category mainly targets the GPU hardware platform and consists of the following tests.</p>
<ol>
<li>GEMM<br>
Matrix-matrix multiplication (GEMM) is a fundamental operation in many scientific, engineering, and machine learning applications. There is a continuing demand to optimize this operation. From this point of view, the performance of GEMM is fundamental capability of an AI accelerator. This test consists of a variaty of differently parameterized GEMM operations, convolution, and memory operations.</li>
</ol>
<h3>Layer-based Benchmarks</h3>
<p>The neural networks are consisted of one or more different layers and each layer involves some commonly used operations. To better investigate the perfomrance of a neural network, it is inevitable to perform the evaluation on single layer performance. The layer-based benchmarks create layers using these important operations and aim at testing the performance of isolated layers on AI accelerators.</p>
<h3>Macro Benchmarks</h3>
<p>The macro benchmarks include the CNN and RNN models. The purpose of this benchmarks to test the performance of complete models with focus on inference. The models weights are obtained by trained on a few hundreds iterations with initial random numbers (it is enough for performance testing purpose). They will be tested in two frameworks: caffe and tensorflow. Caffe models and weights are input to TensorRT and the results are based on TensorRT optimized graph. Tensorflow models are coded and tested in Tensorflow framework. The CNN models need to train a few hundreds iterations get obtain the initial checkpoint files. Later, these files are used in inference test phase.<br>
Our model database tracks the academia and industry innovative development of new algorithm and models all the time. New models could be added if they are satisfied with our selection standards.</p>
<p>The macro benchmarks collect a couple of commonly used models in both academia and industry, including CNN models of ILSVRC champions. Besides, it also include some RNN based application in NLP. This benchmarks are evaluated for their performance of running the complete models to get sense of the performance data for some major applications categories.</p>
<ol>
<li>
<p>Image classification<br>
The CNN models consist of the following below. The caffe with tensorRT and tensorflow implementations are included.</p>
<ul>
<li>googleneti</li>
<li>vgg16</li>
<li>resnet50</li>
<li>resnet152</li>
<li>densenet</li>
</ul>
</li>
<li>
<p>Object detection<br>
2.1 Mask RCNN<br>
As one of the challenge in ILSVRC (ImageNet Large Scale Visual Recognition Challenge), object detection emerged many important and prevailing algorithm also. We collected the Mask RCNN merged in 2017 in our benchmarks. In the Mask RCNN framework, resnet101 is used as backbone in this test framework. 28 images are tested in the inference situation. Each image is duplicated to make larger batch size = n. So total image detected in this test is 28*n.
We collect the code from below.<br>
<a href="https://github.com/matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a></p>
<p>2.2 SSD<br>
Single Shot MultiBox Detector (SSD) is  proposed in 2016 and is being used and customized by many compute vision researchers and engineers. It has quite a lot impact to the object detection area.
We collect the code from below which is re-implementation of original caffe implementation.<br>
<a href="https://github.com/balancap/SSD-Tensorflow">https://github.com/balancap/SSD-Tensorflow</a></p>
</li>
<li>
<p>NMT<br>
Neural machine translation (NMT) is based sequence-to-sequence (seq2seq) models.  The seq2seq models are proposed in 2014 and have improved a variety of tasks such as machine translation, speech recognition, and text summarization. Many researcher and algorithm engineer would directly use or customize it based on this model.<br>
We collect the code from below<br>
<a href="https://github.com/tensorflow/nmt">https://github.com/tensorflow/nmt</a></p>
</li>
<li>
<p>DeepSpeech<br>
Deepspeech is speech recognition framework proposed by Baidu in 2014. This is re-implementation of it in tensorflow by Firefox open source project. Deepspeech core architecture is based on well organized RNN network with some data systhesis techniques that allow users to train the system efficiently.
We collect the code from below<br>
<a href="https://github.com/mozilla/DeepSpeech">https://github.com/mozilla/DeepSpeech</a></p>
</li>
<li>
<p>Deep Interest Network (from Alimama)<br>
Alimama belongs to Alibaba Group, is a leading marketing platform for Big Data with Alibaba Group's core business data. Deep Interest Network (DIN) is developed by Alimama engineer and now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic. The framework addresses the problem of click-through rate (CTR) prediction which is an essential task in industrial applications, such as online advertising. The performance of CTR prediction model has a direct impact on the final revenue and plays a key role in the advertising system. This model plays an important role in Alibaba Group.<br>
Model are contributed by the authors of &quot;Deep Interest Network for Click-Through Rate Prediction&quot;. Thanks for the contribution from Guorui Zhou, Peng Sun, Zelin Hu, etc..<br>
<a href="https://github.com/zhougr1993/DeepInterestNetwork">https://github.com/zhougr1993/DeepInterestNetwork</a></p>
</li>
</ol>
<h3>Synthetic Benchmarks (StatsNet)</h3>
<p>Deep learning (DL) architecture, such as convolutional neural networks (CNN), involves heavy computation and require hardware, such as CPU, GPU, and AI accelerators, to provide the massive computing power. With the many varieties of AI hardware prevailing on the market, it is often hard to decide which one is the best to use. Thus, benchmarking AI hardware effectively becomes important and is of great help to select and optimize AI hardware.</p>
<p>Unfortunately, the current AI benchmarks always suffer some drawbacks of traditional benchmarks. First, they cannot adapt to the emerging changes of DL algorithms and are fixed once selected. Second, they contain tens to hundreds of applications and take very long time to finish running. Third, they are mainly selected from open sources, which are restricted by copyright and are not representable to proprietary applications.</p>
<p>We propose a synthetic benchmarks framework is firstly proposed to address the above drawbacks of AI benchmarks. Instead of pre-selecting a set of open-sourced benchmarks and running all of them, the synthetic approach generates only a one or few benchmarks that best represent a broad range of applications using profiled workload characteristics data of these applications. Thus, it can adapt to emerging changes of new DL algorithms by re-profiling new applications and updating itself, greatly reduce benchmark count and running time, and strongly represent DL applications of interests. The generated benchmarks serve as a performance benchmarks matching the statistical workload characteristics of a combination of applications of interests.</p>
<h1>Suggestions</h1>
<p>We keep working hard to develope our benchmark suites. Any suggestions, contributions and improvements from anyone are welcome. Please do not hesitate to contact us if you want to participate in this open source project. You could submit questions on Github or contact us through <a href="mailto:aimatrix@list.alibaba-inc.com">aimatrix@list.alibaba-inc.com</a></p>
</div></section><footer class="footer-container" data-reactid="34"><div class="footer-body" data-reactid="35"><div class="cols-container" data-reactid="36"><div class="col col-12" data-reactid="37"><h3 data-reactid="38">Disclaimer</h3><p data-reactid="39">the disclaimer content</p></div><div class="col col-6" data-reactid="40"><dl data-reactid="41"><dt data-reactid="42">Documentation</dt><dd data-reactid="43"><a href="/en-us/docs/goals.html" target="_self" data-reactid="44">Goals</a></dd><dd data-reactid="45"><a href="/en-us/docs/contents.html" target="_self" data-reactid="46">Contents</a></dd><dd data-reactid="47"><a href="/en-us/docs/metrics.html" target="_self" data-reactid="48">Metrics</a></dd></dl></div><div class="col col-6" data-reactid="49"><dl data-reactid="50"><dt data-reactid="51">Resources</dt><dd data-reactid="52"><a href="/en-us/blog/index.html" target="_self" data-reactid="53">Blog</a></dd><dd data-reactid="54"><a href="/en-us/community/index.html" target="_self" data-reactid="55">Community</a></dd></dl></div></div><div class="copyright" data-reactid="56"><span data-reactid="57">Copyright © 2018 Alibaba    Contact:aimatrix@list.alibaba-inc.com</span></div></div></footer></div></div>
	<script src="https://f.alicdn.com/react/15.4.1/react-with-addons.min.js"></script>
	<script src="https://f.alicdn.com/react/15.4.1/react-dom.min.js"></script>
	<script>
		window.rootPath = '';
  </script>
	<script src="/build/documentation.js"></script>
</body>
</html>