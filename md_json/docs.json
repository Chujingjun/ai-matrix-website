{
  "zh-cn": [
    {
      "filename": "contents.md",
      "__html": "<h1>测试</h1>\n<h3>底层测试</h3>\n<p>底层测试类包含测量深度学习神经网络里的基本操作性能的测试。这一类主要针对GPU硬件并包含如下测试。</p>\n<ol>\n<li>GEMM<br>\n矩阵乘法是很多科学，工程，和机器学习应用里面的一项基本操作。优化矩阵乘法一直是一个需求。从这个角度看，矩阵乘法的性能是AI加速器能力的表征。这个测试包含一系列不同参数的矩阵乘法，卷积，和内存运算。</li>\n</ol>\n<h3>单层测试</h3>\n<p>神经网络由一个或者多个层组成，每一层包括一些常用的操作。为了更好的调查神经网络的性能，评估单层的性能不可避免。层测试使用一些重要的操作产生不同的层去测试单独的层在AI加速器上的性能。</p>\n<h3>完整测试</h3>\n<p>完整测试包含CNN和RNN模型。这一类测试的目的是测量完整模型的性能，重点在于测试推导的性能。模型的重量参数通过训练这些网络几百个来回得到，训练开始时这些参数被初始化为随机数（这对性能测试足够了）。这些测试有两个软件框架：tensorflow和caffe。Caffe模型和重量参数被送到TensorRT上优化，结果是基于TensorRT优化之后的计算图。Tensorflow模型是在tensorflow框架里编写和测试的。CNN模型先被训练几百个来回以产生初始的检查点文件，这些检查点文件再被用到推导中。我们的模型数据库始终追踪学术界和工业界最新的算法和模型发展。满足筛选条件的新模型会被加入我们的测试中。</p>\n<p>宏测试收集了一些学术届和工业界常用的模型，包括CNN模型和ILSVRC冠军模型。除此以外，还包括一些基于RNN的应用。这些模型被完整的跑下来以评估它们的性能。这些模型包含一下几类。</p>\n<ol>\n<li>图像分类<br>\nCNN模型包含以下模型，分别用tensorRT优化的caffe和tensorflow实现的。\n<ul>\n<li>googlenet</li>\n<li>vgg16</li>\n<li>resnet50</li>\n<li>resnet152</li>\n<li>densenet</li>\n</ul>\n</li>\n<li>物体识别<br>\n2.1 Mask RCNN<br>\n作为ILSCVRC的一项挑战，出现了很多重要的和流行的物体探测的算法。我们在我们的测试中收集了Mask RCNN。在Mask RCNN框架中，resnet101是主要支柱。推导测试了28个图像。每一个图像被复制n次以增大batch size。所以总共探测的图像数量是28n。我们从这里收集Mask RCNN的源代码<br>\n<a href=\"https://github.com/matterport/Mask_RCNN\">https://github.com/matterport/Mask_RCNN</a><br>\n2.2 SSD<br>\nSingle Shot MultiBox Detector (SSD)在2016年被提出并且被许多计算机图形研究者和工程师应用。对物体识别领域有重大影响。我们从这里收集SSD的源代码<br>\n<a href=\"https://github.com/balancap/SSD-Tensorflow\">https://github.com/balancap/SSD-Tensorflow</a></li>\n<li>NMT<br>\n神经机器翻译（NMT）是一个基于序列到序列到模型。这个模型在2014年被提出并且改进了一系列的任务，例如，机器翻译，语音识别，和文本总结。很多研究者和算法工程师会直接使用或者修改这个模型。我们从这里收集到NMT的代码\n<a href=\"https://github.com/tensorflow/nmt\">https://github.com/tensorflow/nmt</a></li>\n<li>DeepSpeech<br>\nDeepSpeech是一个由百度在2014年提出的语音识别框架。这是由Firefox开源项目在tensorflow中重新实现的。 Deepspeech核心架构基于组织良好的RNN网络和一些数据合成技术，允许用户有效地训练系统。 我们从这里收集到DeepSpeech的代码<br>\n<a href=\"https://github.com/mozilla/DeepSpeech\">https://github.com/mozilla/DeepSpeech</a></li>\n<li>Deep Interest Network（来自阿里妈妈）\n阿里妈妈属于阿里巴巴集团，是阿里巴巴集团核心业务数据的大数据领先营销平台。 Deep Interest Network（DIN）由阿里妈妈工程师开发，现已成功部署在阿里巴巴的在线展示广告系统中，为主要流量提供服务。 该框架解决了点击率（CTR）预测的问题，这是工业应用中的重要任务，例如在线广告。 CTR预测模型的性能对最终收入有直接影响，并在广告系统中起关键作用。 该模型在阿里巴巴集团中发挥着重要作用。\n模型由“深度兴趣网络点击率预测”的作者提供。 感谢Guorui Zhou，Peng Sun，Zelin Hu等人的贡献。<br>\n<a href=\"https://github.com/zhougr1993/DeepInterestNetwork\">https://github.com/zhougr1993/DeepInterestNetwork</a></li>\n</ol>\n<h3>合成测试</h3>\n<p>深度学习（DL）架构，例如卷积神经网络（CNN），涉及大量计算并且需要硬件（例如CPU，GPU和AI加速器）来提供巨大的计算能力。 由于市场上普遍存在多种AI硬件，因此通常很难确定哪种硬件最适合使用。 因此，有效地对AI硬件进行基准测试变得非常重要，并且对选择和优化AI硬件有很大帮助。<br>\n不幸的是，目前的AI基准测试总是遇到传统基准测试的一些缺点。 首先，它们无法适应DL算法的新兴变化，并且一旦被选中就固定不变了。 其次，它们包含数十到数百个应用程序，并且需要很长时间才能完成运行。 第三，它们主要选自开源，受版权限制，不能代表专有应用。<br>\n我们提出了一个合成测试模型来解决AI基准测试的上述缺点。 合成方法不是预先选择一组开源测试并运行所有这些测试，而是通过分析各种应用程序的负载特征，仅生成一个或几个测试，这些测试最能代表这些应用程序。 因此，它可以通过重新分析新应用程序和更新自身来适应新DL算法的新变化，大大减少测试数目和运行时间，并能很好的代表DL应用程序。合成测试被用来测试硬件性能。</p>\n<h3>建议</h3>\n<p>我们仍在努力开发我们的基准测试套件。 我们欢迎任何人的任何建议，贡献和改进。 如果您想参与，请不要犹豫与我们联系。 谢谢。 您可以在Github上提交问题或通过w.wei@alibaba-inc.com或者wz.ww@alibaba-inc.com 与我们联系。</p>\n"
    },
    {
      "filename": "goals.md",
      "__html": "<h1>目标</h1>\n<p>AI Matrix是一个测试AI软件框架和硬件平台的标准。它旨在为用户提供一个测量不同AI软件和硬件的方法并且比较它们对好坏。它帮助用户了解影响AI硬件性能的因素并帮助用户改进硬件设计。</p>\n<p><img src=\"./structure.jpg\" alt=\"img\"><br>\nAI Matrix包含四类测试：微测试，层测试，完整测试，和合成测试。测试粒度逐渐增加。微测试着重于AI硬件计算中重要的GEMM计算，层测试着重于评价神经网络里面的每一层，完整测试着重于评价不同应用领域的完整模型。大部分应用是基于Tensorflow，小部分应用是基于caffe。Tensorflow是AI应用领域最重要的软件框架之一，在阿里巴巴内部广泛应用。Caffe是AI软件框架的先行者，有很多模型还是基于caffe框架。因为资源有限，我们只收集了基于这两个软件框架的应用，但是在未来我们会扩展到其他软件框架。</p>\n"
    },
    {
      "filename": "metrics.md",
      "__html": "<h1>标准</h1>\n<p>很多因素会影响AI应用在加速器上的最终性能。但是最基本直观的标准是时钟时间。在我们的测试里，我们用时钟时间作为基本标准。在现有的训练和推导市场里，NVIDIA的产品是一个重要的角色。尽管谷歌TPU被用在公共云中，但是我们目前还没有机会测试。为了给每一个硬件平台打一个分数，我们用NVIDIA的旗舰产品作为基准。我们用P4作为推理的基准（用V100作为训练的基准）。在未来，我们会考虑使用归一化的标准来调整这个值，比如，消耗的能量。但是由于它的测量不是非常稳定，我们仍然在等待更好的条件去归一化这些标准。</p>\n<p>在未来的修订版本中，我们会考虑以下标准作为归一化标准。</p>\n<ul>\n<li>跑一个测试消耗的能量</li>\n<li>硬件资源利用率</li>\n</ul>\n"
    }
  ],
  "en-us": [
    {
      "filename": "contents.md",
      "__html": "<h1>Benchmarks</h1>\n<h3>Micro Benchmarks</h3>\n<p>The micro benchmark category consists of tests that measure the performance of the basic operations involved in training deep learning neural networks. This category mainly targets the GPU hardware platform and consists of the following tests.</p>\n<ol>\n<li>GEMM<br>\nMatrix-matrix multiplication (GEMM) is a fundamental operation in many scientific, engineering, and machine learning applications. There is a continuing demand to optimize this operation. From this point of view, the performance of GEMM is fundamental capability of an AI accelerator. This test consists of a variaty of differently parameterized GEMM operations, convolution, and memory operations.</li>\n</ol>\n<h3>Layer-based Benchmarks</h3>\n<p>The neural networks are consisted of one or more different layers and each layer involves some commonly used operations. To better investigate the perfomrance of a neural network, it is inevitable to perform the evaluation on single layer performance. The layer-based benchmarks create layers using these important operations and aim at testing the performance of isolated layers on AI accelerators.</p>\n<h3>Macro Benchmarks</h3>\n<p>The macro benchmarks include the CNN and RNN models. The purpose of this benchmarks to test the performance of complete models with focus on inference. The models weights are obtained by trained on a few hundreds iterations with initial random numbers (it is enough for performance testing purpose). They will be tested in two frameworks: caffe and tensorflow. Caffe models and weights are input to TensorRT and the results are based on TensorRT optimized graph. Tensorflow models are coded and tested in Tensorflow framework. The CNN models need to train a few hundreds iterations get obtain the initial checkpoint files. Later, these files are used in inference test phase.<br>\nOur model database tracks the academia and industry innovative development of new algorithm and models all the time. New models could be added if they are satisfied with our selection standards.</p>\n<p>The macro benchmarks collect a couple of commonly used models in both academia and industry, including CNN models of ILSVRC champions. Besides, it also include some RNN based application in NLP. This benchmarks are evaluated for their performance of running the complete models to get sense of the performance data for some major applications categories.</p>\n<ol>\n<li>\n<p>Image classification<br>\nThe CNN models consist of the following below. The caffe with tensorRT and tensorflow implementations are included.</p>\n<ul>\n<li>googleneti</li>\n<li>vgg16</li>\n<li>resnet50</li>\n<li>resnet152</li>\n<li>densenet</li>\n</ul>\n</li>\n<li>\n<p>Object detection<br>\n2.1 Mask RCNN<br>\nAs one of the challenge in ILSVRC (ImageNet Large Scale Visual Recognition Challenge), object detection emerged many important and prevailing algorithm also. We collected the Mask RCNN merged in 2017 in our benchmarks. In the Mask RCNN framework, resnet101 is used as backbone in this test framework. 28 images are tested in the inference situation. Each image is duplicated to make larger batch size = n. So total image detected in this test is 28*n.\nWe collect the code from below.<br>\n<a href=\"https://github.com/matterport/Mask_RCNN\">https://github.com/matterport/Mask_RCNN</a></p>\n<p>2.2 SSD<br>\nSingle Shot MultiBox Detector (SSD) is  proposed in 2016 and is being used and customized by many compute vision researchers and engineers. It has quite a lot impact to the object detection area.\nWe collect the code from below which is re-implementation of original caffe implementation.<br>\n<a href=\"https://github.com/balancap/SSD-Tensorflow\">https://github.com/balancap/SSD-Tensorflow</a></p>\n</li>\n<li>\n<p>NMT<br>\nNeural machine translation (NMT) is based sequence-to-sequence (seq2seq) models.  The seq2seq models are proposed in 2014 and have improved a variety of tasks such as machine translation, speech recognition, and text summarization. Many researcher and algorithm engineer would directly use or customize it based on this model.<br>\nWe collect the code from below<br>\n<a href=\"https://github.com/tensorflow/nmt\">https://github.com/tensorflow/nmt</a></p>\n</li>\n<li>\n<p>DeepSpeech<br>\nDeepspeech is speech recognition framework proposed by Baidu in 2014. This is re-implementation of it in tensorflow by Firefox open source project. Deepspeech core architecture is based on well organized RNN network with some data systhesis techniques that allow users to train the system efficiently.\nWe collect the code from below<br>\n<a href=\"https://github.com/mozilla/DeepSpeech\">https://github.com/mozilla/DeepSpeech</a></p>\n</li>\n<li>\n<p>Deep Interest Network (from Alimama)<br>\nAlimama belongs to Alibaba Group, is a leading marketing platform for Big Data with Alibaba Group's core business data. Deep Interest Network (DIN) is developed by Alimama engineer and now has been successfully deployed in the online display advertising system in Alibaba, serving the main traffic. The framework addresses the problem of click-through rate (CTR) prediction which is an essential task in industrial applications, such as online advertising. The performance of CTR prediction model has a direct impact on the final revenue and plays a key role in the advertising system. This model plays an important role in Alibaba Group.<br>\nModel are contributed by the authors of &quot;Deep Interest Network for Click-Through Rate Prediction&quot;. Thanks for the contribution from Guorui Zhou, Peng Sun, Zelin Hu, etc..<br>\n<a href=\"https://github.com/zhougr1993/DeepInterestNetwork\">https://github.com/zhougr1993/DeepInterestNetwork</a></p>\n</li>\n</ol>\n<h3>Synthetic Benchmarks</h3>\n<p>Deep learning (DL) architecture, such as convolutional neural networks (CNN), involves heavy computation and require hardware, such as CPU, GPU, and AI accelerators, to provide the massive computing power. With the many varieties of AI hardware prevailing on the market, it is often hard to decide which one is the best to use. Thus, benchmarking AI hardware effectively becomes important and is of great help to select and optimize AI hardware.</p>\n<p>Unfortunately, the current AI benchmarks always suffer some drawbacks of traditional benchmarks. First, they cannot adapt to the emerging changes of DL algorithms and are fixed once selected. Second, they contain tens to hundreds of applications and take very long time to finish running. Third, they are mainly selected from open sources, which are restricted by copyright and are not representable to proprietary applications.</p>\n<p>We propose a synthetic benchmarks framework is firstly proposed to address the above drawbacks of AI benchmarks. Instead of pre-selecting a set of open-sourced benchmarks and running all of them, the synthetic approach generates only a one or few benchmarks that best represent a broad range of applications using profiled workload characteristics data of these applications. Thus, it can adapt to emerging changes of new DL algorithms by re-profiling new applications and updating itself, greatly reduce benchmark count and running time, and strongly represent DL applications of interests. The generated benchmarks serve as a performance benchmarks matching the statistical workload characteristics of a combination of applications of interests.</p>\n<h1>Suggestions</h1>\n<p>We are still keep working hard to developing our benchmark suites. We are welcome to any suggestions, contributions and improvements from anyone. Please do not hesitate to contact us if you want to involve. Thanks.\nYou could submit questions on Github or contact us through <a href=\"mailto:w.wei@alibaba-inc.com\">w.wei@alibaba-inc.com</a> or <a href=\"mailto:wz.ww@alibaba-inc.com\">wz.ww@alibaba-inc.com</a></p>\n"
    },
    {
      "filename": "demo2.md",
      "__html": "<h1>demo for reference of md or image for relative path</h1>\n<p>if you want to reference to another md, the path can be written in the format of relative path under the circumstance of file system.\nclick here： <a href=\"./demo1.md\">demo1.md</a>。</p>\n<p>the same is to image.\n<img src=\"./img/brhtqqzh.jpeg\" alt=\"\"></p>\n"
    },
    {
      "filename": "dir/demo3.md",
      "__html": "<h1>demo for reference of md or image for relative path, across directory</h1>\n<p>if you want to reference to another md, the path can be written in the format of relative path under the circumstance of file system.\nclick here： <a href=\"../demo1.md\">demo1.md</a>。</p>\n<p>the same is to image.\n<img src=\"../img/brhtqqzh.jpeg\" alt=\"\"></p>\n"
    },
    {
      "filename": "goals.md",
      "__html": "<h1>Goals</h1>\n<p>AI Matrix is a benchmark suite for testing AI software frameworks and hardware platforms. It aims at providing users a means of measuring the performance of different AI sofware and hardware and comparing their pros and cons. It also helps users gain insights into the factors that affect AI hardware performance and improve hardware design.</p>\n<p><img src=\"./structure.jpg\" alt=\"img\"><br>\nThe AI Matrix suite consists of four categories of tests: micro benchmarks, layer-based benchmarks, macro benchmarks, and synthetic benchmarks. The testing granularity of test sub-category is increasing. Micro benchmarks focus on basic hardware level GEMM compuation which is also important to AI computation; layer-based benchmarks focus on evaluating the basic element of neural network; macro benchmarks evaluate the complete models from different application areas. Majority of the application are tested on tensorflow while CNN tests are also evaluated in caffe. Tensorflow is one of the most important framework in AI application area and it widely used in Alibaba also. Caffe is one of pioneer in the AI framework and there are still a lot of legacy models wrtitten in Caffe. Due to our limited resources, we only collect test cases from these two frameworks but in future, we will extend our collection in other frameworks.</p>\n"
    },
    {
      "filename": "metrics.md",
      "__html": "<h1>Metrics</h1>\n<p>There are a lot of factors affect the AI applications' final performance on hard accelerators. However, the fundamental and intuitive metric would be the running time (<strong>wall clock time</strong>). In our test suites, we use wall clock  as the basic metrics. In the current market of inference and training, NVIDIA products play an important role in it. Although Google TPU has been on public cloud but accessibility is still limited. Here to give a score to every test. We propose to use the flagship product of Nvidia as baseline. We use P4 as inference and V100 as training baseline. In future, we consider some normalization metrics to adjust the value. For ex, energy consumption. But as it is till not very reliable, we are still waiting some better condition to normalize these metrics.</p>\n<p>In the coming revised versions, we propose to consider the following metrics as normalization metrics.</p>\n<ul>\n<li>Energy consumption of running a benchmark</li>\n<li>hardware utilization</li>\n</ul>\n"
    }
  ]
}